{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 15:37:02.785265: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-25 15:37:02.785312: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-25 15:37:02.785347: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-25 15:37:02.798648: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 15:37:03.797374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration, MBartTokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "  )\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "\n",
    "all_data_path = '.data/multi30k'\n",
    "train_en_path = all_data_path + '/train.en'\n",
    "train_vi_path = all_data_path + '/train.vi'\n",
    "val_en_path = all_data_path + '/val.en'\n",
    "val_vi_path = all_data_path + '/val.vi'\n",
    "test_en_path = all_data_path + '/test2016.en'\n",
    "test_vi_path = all_data_path + '/test2016.vi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size of data is 1207\n",
      "total size of data is 128134\n"
     ]
    }
   ],
   "source": [
    "def load_test():\n",
    "  data = []\n",
    "  with open(test_en_path) as f1, open(test_vi_path) as f2:\n",
    "      for src, tgt in zip(f1, f2):\n",
    "        data.append(\n",
    "            {\n",
    "                \"translation\": {\n",
    "                    \"en\": src.strip(),\n",
    "                    \"vi\": tgt.strip()\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "  print(f'total size of data is {len(data)}')\n",
    "  tdata = pd.DataFrame(data)\n",
    "  tdata = tdata.reset_index()\n",
    "  tdata = tdata.rename(columns={'index': 'id'})\n",
    "  test = datasets.Dataset.from_pandas(tdata)\n",
    "  return test\n",
    "\n",
    "def load_train():\n",
    "    data = []\n",
    "    with open(train_en_path) as f1, open(train_vi_path) as f2:\n",
    "        for src, tgt in zip(f1, f2):\n",
    "          data.append(\n",
    "              {\n",
    "                  \"translation\": {\n",
    "                      \"en\": src.strip(),\n",
    "                      \"vi\": tgt.strip()\n",
    "                  }\n",
    "              }\n",
    "          )\n",
    "    print(f'total size of data is {len(data)}')\n",
    "    tdata = pd.DataFrame(data)\n",
    "    tdata = tdata.reset_index()\n",
    "    tdata = tdata.rename(columns={'index': 'id'})\n",
    "    dataset = datasets.Dataset.from_pandas(tdata)\n",
    "\n",
    "    # don't care about test_size. \n",
    "    downsampled_dataset = dataset.train_test_split(\n",
    "        train_size=20000, test_size=2000, seed=69\n",
    "    )\n",
    "        \n",
    "    return downsampled_dataset\n",
    "test_set = load_test()\n",
    "train_set = load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[250004, 35378, 136, 17, 1884, 17265, 38, 2], [250004, 17, 1380, 17, 1957, 17, 158, 27227, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "checkpoint=\"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "print(tokenizer([\"Hello and i like money!\", \"i come i see i conquer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 100\n",
    "max_target_length = 100\n",
    "source_lang = \"en\"\n",
    "target_lang = \"vi\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    # no need this line \n",
    "    # with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b10a13e569445c808d3048dc97be92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ae2a9c7e1d469e85d5219264d27c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbd3e1e39b64b02a534d6ad461fb40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = train_set.map(preprocess_function, batched=True)\n",
    "tokenized_test_set = test_set.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99433/985486967.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k64t/_conda/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a MBart50TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 2:29:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.314066</td>\n",
       "      <td>33.591800</td>\n",
       "      <td>30.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.355187</td>\n",
       "      <td>33.312100</td>\n",
       "      <td>30.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.407682</td>\n",
       "      <td>32.791800</td>\n",
       "      <td>30.173200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.481415</td>\n",
       "      <td>32.182300</td>\n",
       "      <td>30.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.526963</td>\n",
       "      <td>31.979300</td>\n",
       "      <td>30.205500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6250, training_loss=0.805046015625, metrics={'train_runtime': 8946.6471, 'train_samples_per_second': 11.177, 'train_steps_per_second': 0.699, 'total_flos': 1.1469669742411776e+16, 'train_loss': 0.805046015625, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(output_dir=\"./mbart_EnglistToVietnamese/\",\n",
    "                        do_train=True,\n",
    "                        do_eval=True,\n",
    "                        evaluation_strategy=\"epoch\",\n",
    "                        per_device_train_batch_size=16,\n",
    "                        per_device_eval_batch_size=16,\n",
    "#                         gradient_accumulation_steps=4,\n",
    "                        learning_rate=2e-5,\n",
    "                        num_train_epochs=5,\n",
    "                        predict_with_generate=True,\n",
    "                        logging_dir=\"/logs\",\n",
    "                        logging_steps=10000,\n",
    "                        save_steps=10000,\n",
    "                        report_to=\"none\"\n",
    "                        )\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args,\n",
    "                data_collator=data_collator, \n",
    "                train_dataset=tokenized_datasets['train'],\n",
    "#                eval_dataset=tokenized_datasets['test'],\n",
    "                 eval_dataset=tokenized_test_set,\n",
    "                tokenizer=tokenizer,\n",
    "              compute_metrics=compute_metrics\n",
    "                )\n",
    "\n",
    "tokenized_datasets, tokenized_test_set, trainer = accelerator.prepare(\n",
    "     tokenized_datasets, tokenized_test_set, trainer\n",
    "      )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_model(name):\n",
    "    MODEL_PATH = Path('models')\n",
    "    MODEL_NAME = Path(name + '.pth')\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "    #print(f'Saving model to : {MODEL_SAVE_PATH}')\n",
    "    torch.save(obj = model.state_dict(),\n",
    "            f = MODEL_SAVE_PATH)\n",
    "\n",
    "save_model('mbart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
